{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"spark-nlp\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.11:2.4.5\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-40-93.ec2.internal:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>spark-nlp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fc0183b6bd0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-40-93.ec2.internal:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>spark-nlp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=spark-nlp>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = spark.sparkContext \n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.json(\"s3://502-project/amazon_game_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(\"helpful\").drop(\"reviewerID\").drop(\"unixReviewTime\").drop(\"reviewTime\").drop(\"reviewerName\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "data = data.withColumn(\"overall\", data[\"overall\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------------+--------------------+\n",
      "|      asin|overall|          reviewText|             summary|\n",
      "+----------+-------+--------------------+--------------------+\n",
      "|B00002STAU|      5|this is a old cla...|           a classic|\n",
      "|B00002STAU|      4|This game is more...|  good fighting game|\n",
      "|B00002STAU|      5|If you love WWF n...|WWF Wrestlemania ...|\n",
      "|B00002STAU|      4|I had WWF Wrestle...|wrestling game wi...|\n",
      "|B00002STAU|      4|I have to admit I...|           A Classic|\n",
      "|B00002STAU|      5|This game was ama...|The Best Wrestlin...|\n",
      "|B00002STAU|      4|This right here i...|wrestling at it's...|\n",
      "|B00002SVP7|      3|The Rampage Editi...|A few new levels ...|\n",
      "|B00002SVP7|      2|Remember the mome...|                WTF?|\n",
      "|B00002SVP7|      2|Back in 1993 Sega...|           Bo-oring!|\n",
      "+----------+-------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall=1 or 2, attitue =1 (negative) \n",
    "# overall=3, attitue =2 (neural)\n",
    "# overall=4 or 5, attitue =3 (positive)\n",
    "data=data.withColumn('attitude', F.when(F.col('overall')<3,1).otherwise(F.when( F.col('overall') == 3,2).otherwise(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------------+--------------------+--------+\n",
      "|      asin|overall|          reviewText|             summary|attitude|\n",
      "+----------+-------+--------------------+--------------------+--------+\n",
      "|B00002STAU|      5|this is a old cla...|           a classic|       3|\n",
      "|B00002STAU|      4|This game is more...|  good fighting game|       3|\n",
      "|B00002STAU|      5|If you love WWF n...|WWF Wrestlemania ...|       3|\n",
      "|B00002STAU|      4|I had WWF Wrestle...|wrestling game wi...|       3|\n",
      "|B00002STAU|      4|I have to admit I...|           A Classic|       3|\n",
      "+----------+-------+--------------------+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "data = data.withColumn(\"attitude\", data[\"attitude\"].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop(\"overall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.withColumn(\"overall\", data[\"attitude\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop(\"attitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+-------+\n",
      "|      asin|          reviewText|             summary|overall|\n",
      "+----------+--------------------+--------------------+-------+\n",
      "|B00002STAU|this is a old cla...|           a classic|    3.0|\n",
      "|B00002STAU|This game is more...|  good fighting game|    3.0|\n",
      "|B00002STAU|If you love WWF n...|WWF Wrestlemania ...|    3.0|\n",
      "|B00002STAU|I had WWF Wrestle...|wrestling game wi...|    3.0|\n",
      "|B00002STAU|I have to admit I...|           A Classic|    3.0|\n",
      "+----------+--------------------+--------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data cleaning-nlp pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/hadoop/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/hadoop/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of stopwords from nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "eng_stopwords = stopwords.words('english')\n",
    "eng_stopwords.append('xxxx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import Finisher, DocumentAssembler\n",
    "from sparknlp.annotator import (Tokenizer, Normalizer, \n",
    "                                LemmatizerModel, StopWordsCleaner)\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "documentAssembler = DocumentAssembler() \\\n",
    "    .setInputCol('reviewText') \\\n",
    "    .setOutputCol('document')\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols(['document']) \\\n",
    "    .setOutputCol('token')\n",
    "\n",
    "# note normalizer defaults to changing all words to lowercase.\n",
    "# Use .setLowercase(False) to maintain input case.\n",
    "normalizer = Normalizer() \\\n",
    "    .setInputCols(['token']) \\\n",
    "    .setOutputCol('normalized') \\\n",
    "    .setLowercase(True)\n",
    "\n",
    "# note that lemmatizer needs a dictionary. So I used the pre-trained\n",
    "# model (note that it defaults to english)\n",
    "lemmatizer = LemmatizerModel.pretrained() \\\n",
    "    .setInputCols(['normalized']) \\\n",
    "    .setOutputCol('lemma') \\\n",
    "\n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "    .setInputCols(['lemma']) \\\n",
    "    .setOutputCol('clean_lemma') \\\n",
    "    .setCaseSensitive(False) \\\n",
    "    .setStopWords(eng_stopwords)\n",
    "\n",
    "# finisher converts tokens to human-readable output\n",
    "finisher = Finisher() \\\n",
    "    .setInputCols(['clean_lemma']) \\\n",
    "    .setCleanAnnotations(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline() \\\n",
    "    .setStages([\n",
    "        documentAssembler,\n",
    "        tokenizer,\n",
    "        normalizer,\n",
    "        lemmatizer,\n",
    "        stopwords_cleaner,\n",
    "        finisher\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|          reviewText|\n",
      "+--------------------+\n",
      "|this is a old cla...|\n",
      "|This game is more...|\n",
      "|If you love WWF n...|\n",
      "|I had WWF Wrestle...|\n",
      "|I have to admit I...|\n",
      "|This game was ama...|\n",
      "|This right here i...|\n",
      "|The Rampage Editi...|\n",
      "|Remember the mome...|\n",
      "|Back in 1993 Sega...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.createOrReplaceTempView(\"data\")\n",
    "df = spark.sql(\"SELECT reviewText FROM data\")\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|finished_clean_lemma|\n",
      "+--------------------+\n",
      "|[old, classic, wr...|\n",
      "|[game, oneonone, ...|\n",
      "|[love, wwf, call,...|\n",
      "|[wwf, wrestlemani...|\n",
      "|[admit, hadnt, st...|\n",
      "|[game, amazing, b...|\n",
      "|[right, bit, arca...|\n",
      "|[rampage, edition...|\n",
      "|[remember, moment...|\n",
      "|[back, sega, rele...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "equifax = pipeline.fit(df).transform(df)\n",
    "temp = equifax.select('finished_clean_lemma')\n",
    "temp.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+-------+--------------------+\n",
      "|      asin|          reviewText|             summary|overall|finished_clean_lemma|\n",
      "+----------+--------------------+--------------------+-------+--------------------+\n",
      "|B00002STAU|this is a old cla...|           a classic|    3.0|[old, classic, wr...|\n",
      "|B00002STAU|This game is more...|  good fighting game|    3.0|[game, oneonone, ...|\n",
      "|B00002STAU|If you love WWF n...|WWF Wrestlemania ...|    3.0|[love, wwf, call,...|\n",
      "|B00002STAU|I had WWF Wrestle...|wrestling game wi...|    3.0|[wwf, wrestlemani...|\n",
      "|B00002STAU|I have to admit I...|           A Classic|    3.0|[admit, hadnt, st...|\n",
      "|B00002STAU|This game was ama...|The Best Wrestlin...|    3.0|[game, amazing, b...|\n",
      "|B00002STAU|This right here i...|wrestling at it's...|    3.0|[right, bit, arca...|\n",
      "|B00002SVP7|The Rampage Editi...|A few new levels ...|    2.0|[rampage, edition...|\n",
      "|B00002SVP7|Remember the mome...|                WTF?|    1.0|[remember, moment...|\n",
      "|B00002SVP7|Back in 1993 Sega...|           Bo-oring!|    1.0|[back, sega, rele...|\n",
      "+----------+--------------------+--------------------+-------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id, row_number\n",
    "from pyspark.sql.window import Window\n",
    "data=data.withColumn('row_index', row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "temp=temp.withColumn('row_index', row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "data = data.join(temp, on=[\"row_index\"]).drop(\"row_index\")\n",
    "data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[asin: string, reviewText: string, summary: string, overall: double, finished_clean_lemma: array<string>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training records: 860037\n",
      "Number of testing records : 215275\n"
     ]
    }
   ],
   "source": [
    "splitted_data = data.randomSplit([0.8, 0.2])\n",
    "train_data = splitted_data[0]\n",
    "test_data = splitted_data[1]\n",
    "\n",
    "print(\"Number of training records: \" + str(train_data.count()))\n",
    "print(\"Number of testing records : \" + str(test_data.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+-------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|      asin|          reviewText|             summary|overall|finished_clean_lemma|               words|                  tf|            features|label|\n",
      "+----------+--------------------+--------------------+-------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|0700099867|1st shipment rece...|           Wrong key|    1.0|[st, shipment, re...|[1st, shipment, r...|(65536,[568,6534,...|(65536,[568,6534,...|  1.0|\n",
      "|0700099867|Although this gam...|Great game, Bad Host|    2.0|[although, game, ...|[although, this, ...|(65536,[1924,2026...|(65536,[1924,2026...|  2.0|\n",
      "|0700099867|Amazing graphics,...|         Great Game!|    3.0|[amazing, graphic...|[amazing, graphic...|(65536,[8026,1165...|(65536,[8026,1165...|  0.0|\n",
      "|0700099867|Crashed in Vista....|Don't waste your ...|    1.0|[crash, vista, co...|[crashed, in, vis...|(65536,[4775,8315...|(65536,[4775,8315...|  1.0|\n",
      "|0700099867|DVD came from Eng...|       Dirt 3 for PC|    3.0|[dvd, come, engla...|[dvd, came, from,...|(65536,[308,3294,...|(65536,[308,3294,...|  0.0|\n",
      "+----------+--------------------+--------------------+-------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"reviewText\", outputCol=\"words\")\n",
    "hashtf = HashingTF(numFeatures=2**16, inputCol=\"words\", outputCol='tf')\n",
    "idf = IDF(inputCol='tf', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "label_stringIdx = StringIndexer(inputCol = \"overall\", outputCol = \"label\")\n",
    "pipeline = Pipeline(stages=[tokenizer, hashtf, idf, label_stringIdx])\n",
    "\n",
    "pipelineFit = pipeline.fit(train_data)\n",
    "train_df = pipelineFit.transform(train_data)\n",
    "test_df = pipelineFit.transform(test_data)\n",
    "train_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "lrModel = lr.fit(train_df)\n",
    "predictions = lrModel.transform(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[asin: string, reviewText: string, summary: string, overall: double, finished_clean_lemma: array<string>, words: array<string>, tf: vector, features: vector, label: double, rawPrediction: vector, probability: vector, prediction: double]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7592238937529705"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Countvector approach (Logistic Regression) with cross validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr1 = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_stringIdx = StringIndexer(inputCol = \"overall\", outputCol = \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "countVectors = CountVectorizer(inputCol=\"finished_clean_lemma\", outputCol=\"features\", vocabSize=10000, minDF=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+-------+--------------------+\n",
      "|      asin|          reviewText|             summary|overall|finished_clean_lemma|\n",
      "+----------+--------------------+--------------------+-------+--------------------+\n",
      "|B00002STAU|this is a old cla...|           a classic|    3.0|[old, classic, wr...|\n",
      "|B00002STAU|This game is more...|  good fighting game|    3.0|[game, oneonone, ...|\n",
      "|B00002STAU|If you love WWF n...|WWF Wrestlemania ...|    3.0|[love, wwf, call,...|\n",
      "|B00002STAU|I had WWF Wrestle...|wrestling game wi...|    3.0|[wwf, wrestlemani...|\n",
      "|B00002STAU|I have to admit I...|           A Classic|    3.0|[admit, hadnt, st...|\n",
      "+----------+--------------------+--------------------+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[countVectors, label_stringIdx])\n",
    "pipelineFit = pipeline.fit(data)\n",
    "dataset = pipelineFit.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training records: 860648\n",
      "Number of testing records : 214664\n"
     ]
    }
   ],
   "source": [
    "(trainingData, testData) = dataset.randomSplit([0.8, 0.2], seed = 100)\n",
    "print(\"Number of training records: \" + str(trainingData.count()))\n",
    "print(\"Number of testing records : \" + str(testData.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr1.regParam, [0.1, 0.3, 0.5]) # regularization parameter\n",
    "             .addGrid(lr1.elasticNetParam, [0.0, 0.1, 0.2]) # Elastic Net Parameter (Ridge = 0)\n",
    "             .build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CrossValidator(estimator=lr1, \\\n",
    "                    estimatorParamMaps=paramGrid, \\\n",
    "                    evaluator=MulticlassClassificationEvaluator(), \\\n",
    "                    numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel = cv.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7760771669407684"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = cvModel.transform(testData)\n",
    "# Evaluate best model\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
