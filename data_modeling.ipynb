{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Video Games Review Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"spark-nlp\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.11:2.4.5\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-19-168.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>spark-nlp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fa7589b2c10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-19-168.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>spark-nlp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=spark-nlp>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = spark.sparkContext \n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.json(\"s3://qianyielva/amazon_game_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------+--------------------+-----------+--------------+--------------------+--------------------+--------------+\n",
      "|      asin|helpful|overall|          reviewText| reviewTime|    reviewerID|        reviewerName|             summary|unixReviewTime|\n",
      "+----------+-------+-------+--------------------+-----------+--------------+--------------------+--------------------+--------------+\n",
      "|B00002STAU| [0, 0]|    5.0|this is a old cla...|07 30, 2012|A1G0VFQ9198IUF|                  al|           a classic|    1343606400|\n",
      "|B00002STAU| [2, 2]|    4.0|This game is more...|02 21, 2001| AXUOVXIGF9CKC|      \"bigdcaldavis\"|  good fighting game|     982713600|\n",
      "|B00002STAU| [0, 0]|    5.0|If you love WWF n...|11 14, 2011|A15JTJXQXO22JJ|           Chad Frey|WWF Wrestlemania ...|    1321228800|\n",
      "|B00002STAU| [1, 1]|    4.0|I had WWF Wrestle...|08 10, 2008| ANRNG7OAARR70|D. Hensley \"Horro...|wrestling game wi...|    1218326400|\n",
      "|B00002STAU| [0, 0]|    4.0|I have to admit I...|07 24, 2009|A2ZFYB6WY3RG93|        Raqel Redfox|           A Classic|    1248393600|\n",
      "|B00002STAU| [1, 1]|    5.0|This game was ama...|02 27, 2013|A3J8ABVGK7ZL6H|         SideshowBob|The Best Wrestlin...|    1361923200|\n",
      "|B00002STAU| [0, 0]|    4.0|This right here i...| 04 1, 2014|A3DE438TF1A958|        thomas henry|wrestling at it's...|    1396310400|\n",
      "|B00002SVP7| [0, 0]|    3.0|The Rampage Editi...|04 16, 2013|A2CJJSS6OD4K2G|     anita reichmann|A few new levels ...|    1366070400|\n",
      "|B00002SVP7|[7, 13]|    2.0|Remember the mome...| 05 3, 2006|A2ICW5OUWX2A2V|                 Ian|                WTF?|    1146614400|\n",
      "|B00002SVP7| [2, 7]|    2.0|Back in 1993 Sega...|06 20, 2004|A319SKSB556033|Inspector Gadget ...|           Bo-oring!|    1087689600|\n",
      "+----------+-------+-------+--------------------+-----------+--------------+--------------------+--------------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------------+--------------------+\n",
      "|      asin|overall|          reviewText|             summary|\n",
      "+----------+-------+--------------------+--------------------+\n",
      "|B00002STAU|    5.0|this is a old cla...|           a classic|\n",
      "|B00002STAU|    4.0|This game is more...|  good fighting game|\n",
      "|B00002STAU|    5.0|If you love WWF n...|WWF Wrestlemania ...|\n",
      "|B00002STAU|    4.0|I had WWF Wrestle...|wrestling game wi...|\n",
      "|B00002STAU|    4.0|I have to admit I...|           A Classic|\n",
      "|B00002STAU|    5.0|This game was ama...|The Best Wrestlin...|\n",
      "|B00002STAU|    4.0|This right here i...|wrestling at it's...|\n",
      "|B00002SVP7|    3.0|The Rampage Editi...|A few new levels ...|\n",
      "|B00002SVP7|    2.0|Remember the mome...|                WTF?|\n",
      "|B00002SVP7|    2.0|Back in 1993 Sega...|           Bo-oring!|\n",
      "+----------+-------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = data.drop(\"helpful\").drop(\"reviewerID\").drop(\"unixReviewTime\").drop(\"reviewTime\").drop(\"reviewerName\")\n",
    "data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "data = data.withColumn(\"overall\", data[\"overall\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------------+--------------------+\n",
      "|      asin|overall|          reviewText|             summary|\n",
      "+----------+-------+--------------------+--------------------+\n",
      "|B00002STAU|      5|this is a old cla...|           a classic|\n",
      "|B00002STAU|      4|This game is more...|  good fighting game|\n",
      "|B00002STAU|      5|If you love WWF n...|WWF Wrestlemania ...|\n",
      "|B00002STAU|      4|I had WWF Wrestle...|wrestling game wi...|\n",
      "|B00002STAU|      4|I have to admit I...|           A Classic|\n",
      "|B00002STAU|      5|This game was ama...|The Best Wrestlin...|\n",
      "|B00002STAU|      4|This right here i...|wrestling at it's...|\n",
      "|B00002SVP7|      3|The Rampage Editi...|A few new levels ...|\n",
      "|B00002SVP7|      2|Remember the mome...|                WTF?|\n",
      "|B00002SVP7|      2|Back in 1993 Sega...|           Bo-oring!|\n",
      "+----------+-------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[asin: string, overall: int, reviewText: string, summary: string]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Reviews in the Dataset: 1075312\n"
     ]
    }
   ],
   "source": [
    "# Size of the Review Tables (Row Count)\n",
    "print(\"Total Number of Reviews in the Dataset:\", data.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Column Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Schema (Column Names)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['asin', 'overall', 'reviewText', 'summary']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print Column Names\n",
    "print(\"Dataset Schema (Column Names)\")\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the Number of Reviews by Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|score|number|\n",
      "+-----+------+\n",
      "|    2| 59477|\n",
      "|    3| 97153|\n",
      "|    1|112789|\n",
      "|    4|210922|\n",
      "|    5|594971|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Plot the Number of Reviews by Score\n",
    "data.createOrReplaceTempView(\"data\")\n",
    "count = spark.sql(\"SELECT overall as score, count(overall) as number FROM data GROUP BY overall ORDER BY number\")\n",
    "count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fa734241190>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "count = count.toPandas()\n",
    "count.plot(kind='bar',x='score',y='number')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show First 10 Products with The Most Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|   product|number|\n",
      "+----------+------+\n",
      "|B00DJFIMW6| 16221|\n",
      "|B00BGA9WK2|  7561|\n",
      "|B00FAX6XQC|  5713|\n",
      "|B009KS4XRO|  5489|\n",
      "|B002VBWIP6|  5190|\n",
      "|B0055SWM08|  4638|\n",
      "|B00CSR2J9I|  4510|\n",
      "|B0015AARJI|  4468|\n",
      "|B00178630A|  3522|\n",
      "|B000FKBCX4|  3290|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "product_review_number = spark.sql(\"SELECT asin as product, count(reviewText) as number FROM data GROUP BY asin ORDER BY number DESC LIMIT 10\")\n",
    "product_review_number.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Review Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/hadoop/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/hadoop/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of stopwords from nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "eng_stopwords = stopwords.words('english')\n",
    "eng_stopwords.append('xxxx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up SparkNLP pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import Finisher, DocumentAssembler\n",
    "from sparknlp.annotator import (Tokenizer, Normalizer, \n",
    "                                LemmatizerModel, StopWordsCleaner)\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "documentAssembler = DocumentAssembler() \\\n",
    "    .setInputCol('reviewText') \\\n",
    "    .setOutputCol('document')\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols(['document']) \\\n",
    "    .setOutputCol('token')\n",
    "\n",
    "# note normalizer defaults to changing all words to lowercase.\n",
    "# Use .setLowercase(False) to maintain input case.\n",
    "normalizer = Normalizer() \\\n",
    "    .setInputCols(['token']) \\\n",
    "    .setOutputCol('normalized') \\\n",
    "    .setLowercase(True)\n",
    "\n",
    "# note that lemmatizer needs a dictionary. So I used the pre-trained\n",
    "# model (note that it defaults to english)\n",
    "lemmatizer = LemmatizerModel.pretrained() \\\n",
    "    .setInputCols(['normalized']) \\\n",
    "    .setOutputCol('lemma') \\\n",
    "\n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "    .setInputCols(['lemma']) \\\n",
    "    .setOutputCol('clean_lemma') \\\n",
    "    .setCaseSensitive(False) \\\n",
    "    .setStopWords(eng_stopwords)\n",
    "\n",
    "# finisher converts tokens to human-readable output\n",
    "finisher = Finisher() \\\n",
    "    .setInputCols(['clean_lemma']) \\\n",
    "    .setCleanAnnotations(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline() \\\n",
    "    .setStages([\n",
    "        documentAssembler,\n",
    "        tokenizer,\n",
    "        normalizer,\n",
    "        lemmatizer,\n",
    "        stopwords_cleaner,\n",
    "        finisher\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|          reviewText|\n",
      "+--------------------+\n",
      "|this is a old cla...|\n",
      "|This game is more...|\n",
      "|If you love WWF n...|\n",
      "|I had WWF Wrestle...|\n",
      "|I have to admit I...|\n",
      "|This game was ama...|\n",
      "|This right here i...|\n",
      "|The Rampage Editi...|\n",
      "|Remember the mome...|\n",
      "|Back in 1993 Sega...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.createOrReplaceTempView(\"data\")\n",
    "df = spark.sql(\"SELECT reviewText FROM data\")\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|finished_clean_lemma|\n",
      "+--------------------+\n",
      "|[old, classic, wr...|\n",
      "|[game, oneonone, ...|\n",
      "|[love, wwf, call,...|\n",
      "|[wwf, wrestlemani...|\n",
      "|[admit, hadnt, st...|\n",
      "|[game, amazing, b...|\n",
      "|[right, bit, arca...|\n",
      "|[rampage, edition...|\n",
      "|[remember, moment...|\n",
      "|[back, sega, rele...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "equifax = pipeline.fit(df).transform(df)\n",
    "temp = equifax.select('finished_clean_lemma')\n",
    "temp.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------------+--------------------+--------------------+\n",
      "|      asin|overall|          reviewText|             summary|finished_clean_lemma|\n",
      "+----------+-------+--------------------+--------------------+--------------------+\n",
      "|B00002STAU|      5|this is a old cla...|           a classic|[old, classic, wr...|\n",
      "|B00002STAU|      4|This game is more...|  good fighting game|[game, oneonone, ...|\n",
      "|B00002STAU|      5|If you love WWF n...|WWF Wrestlemania ...|[love, wwf, call,...|\n",
      "|B00002STAU|      4|I had WWF Wrestle...|wrestling game wi...|[wwf, wrestlemani...|\n",
      "|B00002STAU|      4|I have to admit I...|           A Classic|[admit, hadnt, st...|\n",
      "|B00002STAU|      5|This game was ama...|The Best Wrestlin...|[game, amazing, b...|\n",
      "|B00002STAU|      4|This right here i...|wrestling at it's...|[right, bit, arca...|\n",
      "|B00002SVP7|      3|The Rampage Editi...|A few new levels ...|[rampage, edition...|\n",
      "|B00002SVP7|      2|Remember the mome...|                WTF?|[remember, moment...|\n",
      "|B00002SVP7|      2|Back in 1993 Sega...|           Bo-oring!|[back, sega, rele...|\n",
      "+----------+-------+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id, row_number\n",
    "from pyspark.sql.window import Window\n",
    "data=data.withColumn('row_index', row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "temp=temp.withColumn('row_index', row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "data = data.join(temp, on=[\"row_index\"]).drop(\"row_index\")\n",
    "data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[asin: string, overall: int, reviewText: string, summary: string, finished_clean_lemma: array<string>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset into training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training records: 860190\n",
      "Number of testing records : 215122\n"
     ]
    }
   ],
   "source": [
    "splitted_data = data.randomSplit([0.8, 0.2])\n",
    "train_data = splitted_data[0]\n",
    "test_data = splitted_data[1]\n",
    "\n",
    "print(\"Number of training records: \" + str(train_data.count()))\n",
    "print(\"Number of testing records : \" + str(test_data.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Modeling Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, IndexToString, VectorAssembler, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline, Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build LogisticRegression model(s) and train them using pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "countVectors = CountVectorizer(inputCol=\"finished_clean_lemma\", outputCol=\"features\", vocabSize=10000, minDF=5)\n",
    "stringIndexer_label = StringIndexer(inputCol=\"overall\", outputCol=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = LogisticRegression(labelCol=\"label\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelConverter = IndexToString(inputCol=\"prediction\", \n",
    "                               outputCol=\"predictedLabel\", \n",
    "                               labels=stringIndexer_label.fit(train_data).labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_logit = Pipeline(stages=[\n",
    "                               countVectors,\n",
    "                               stringIndexer_label,\n",
    "                               logit,\n",
    "                               labelConverter])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model using training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_model= pipeline_logit.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the predictions on test data using the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = logit_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- asin: string (nullable = true)\n",
      " |-- overall: integer (nullable = true)\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- finished_clean_lemma: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- label: double (nullable = false)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      " |-- predictedLabel: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score is:  0.5966646543569462\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "print(\"Accuracy score is: \", evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modeling Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, IndexToString, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline, Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringIndexer_label = StringIndexer(inputCol=\"overall\", outputCol=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringIndexer_feature = StringIndexer(inputCol=\"reviewText\", outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset into training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training records: 859944\n",
      "Number of testing records : 215368\n"
     ]
    }
   ],
   "source": [
    "splitted_data = data.randomSplit([0.8, 0.2])\n",
    "train_data = splitted_data[0]\n",
    "test_data = splitted_data[1]\n",
    "\n",
    "print(\"Number of training records: \" + str(train_data.count()))\n",
    "print(\"Number of testing records : \" + str(test_data.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build LogisticRegression model(s) and train them using pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = LogisticRegression(labelCol=\"label\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelConverter = IndexToString(inputCol=\"prediction\", \n",
    "                               outputCol=\"predictedLabel\", \n",
    "                               labels=stringIndexer_label.fit(train_data).labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_logit = Pipeline(stages=[stringIndexer_label,stringIndexer_feature,logit,labelConverter])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model using training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "'requirement failed: Column features must be of type struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually double.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o114.fit.\n: java.lang.IllegalArgumentException: requirement failed: Column features must be of type struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually double.\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.util.SchemaUtils$.checkColumnType(SchemaUtils.scala:43)\n\tat org.apache.spark.ml.PredictorParams$class.validateAndTransformSchema(Predictor.scala:51)\n\tat org.apache.spark.ml.classification.Classifier.org$apache$spark$ml$classification$ClassifierParams$$super$validateAndTransformSchema(Classifier.scala:58)\n\tat org.apache.spark.ml.classification.ClassifierParams$class.validateAndTransformSchema(Classifier.scala:42)\n\tat org.apache.spark.ml.classification.ProbabilisticClassifier.org$apache$spark$ml$classification$ProbabilisticClassifierParams$$super$validateAndTransformSchema(ProbabilisticClassifier.scala:53)\n\tat org.apache.spark.ml.classification.ProbabilisticClassifierParams$class.validateAndTransformSchema(ProbabilisticClassifier.scala:37)\n\tat org.apache.spark.ml.classification.LogisticRegression.org$apache$spark$ml$classification$LogisticRegressionParams$$super$validateAndTransformSchema(LogisticRegression.scala:279)\n\tat org.apache.spark.ml.classification.LogisticRegressionParams$class.validateAndTransformSchema(LogisticRegression.scala:266)\n\tat org.apache.spark.ml.classification.LogisticRegression.validateAndTransformSchema(LogisticRegression.scala:279)\n\tat org.apache.spark.ml.Predictor.transformSchema(Predictor.scala:144)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:100)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:82)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-94735b5ea3f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlogit_model\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpipeline_logit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    107\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: 'requirement failed: Column features must be of type struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually double.'"
     ]
    }
   ],
   "source": [
    "logit_model= pipeline_logit.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
